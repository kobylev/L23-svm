# SVM Classification Laboratory

**Lab: Support Vector Machines in Practice**

A comprehensive Python-based laboratory system demonstrating the implementation and comparison of Support Vector Machine (SVM) classifiers using both standard libraries and manual implementation from first principles.

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Phases](#phases)
- [Analysis and Conclusions](#analysis-and-conclusions)
- [Installation](#installation)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [Results](#results)
- [Educational Value](#educational-value)
- [Module Documentation](#module-documentation)
- [Technical Implementation](#technical-implementation)
- [Testing](#testing)
- [Contributing](#contributing)
- [License](#license)

---

## ğŸ¯ Overview

This laboratory explores the fundamental principles of Support Vector Machines (SVM) through two distinct phases:

1.  **Baseline Establishment** - Utilizing `scikit-learn` to set a high-performance benchmark.
2.  **Manual Implementation** - Building a linear SVM solver from scratch using Gradient Descent and Hinge Loss, and solving a multi-class problem via recursive binary decomposition.

### Key Features

âœ… **Educational Focus** - Demonstrates the math behind SVMs (Gradient Descent, Hinge Loss)
âœ… **Clean Architecture** - Modular design with clear separation of concerns
âœ… **Comprehensive Logging** - Detailed tracking of model training and accuracy
âœ… **Visual Analytics** - Professional decision boundary plots
âœ… **Manual Optimization** - Custom implementation of the SVM optimization algorithm
âœ… **Recursive Logic** - "Divide and Conquer" strategy for multi-class classification

---

## ğŸ§ª Phases

### Phase I: Standard Library Implementation

**Objective**: Establish a performance baseline using industry-standard tools.

**What it does**:
- Loads the Iris dataset
- Trains an `SVC` model with a linear kernel using `scikit-learn`
- Evaluates accuracy on a test set
- **Expected Result**: High accuracy (~97%) and clear decision boundaries

**Output**:
- `decision_boundary_phase_one.png` - Visualization of the sklearn model's decision regions
- Logged accuracy metrics

---

### Phase II: Manual Optimization (Advanced)

**Objective**: Demonstrate deep understanding of SVM principles by implementing the solver manually.

**What it does**:
- Implements a linear SVM class (`ManualSVM`) using Gradient Descent optimization on the Hinge Loss function.
- Implements a `RecursiveBinaryClassifier` to handle the 3-class Iris problem:
    - **Split 1**: Separates Class 0 (Setosa) from Classes 1 & 2.
    - **Split 2**: Separates Class 1 (Versicolor) from Class 2 (Virginica).
- **Expected Result**: Accuracy comparable to Phase I (>90%) using pure custom logic.

**Output**:
- `decision_boundary_phase_two.png` - Visualization of the manual model's decision regions
- Logged accuracy metrics confirming success

---

## ğŸ“Š Analysis and Conclusions

### Data Visualization

**Observation**: The Iris dataset contains three classes. Class 0 (Setosa) is linearly separable from the others. Classes 1 (Versicolor) and 2 (Virginica) are closer together, presenting a harder classification challenge.

![Iris Data Visualization](plots/iris_data_visualization.png)

### Phase I: Scikit-Learn Decision Boundary

**Conclusion**: The standard library implementation effectively finds the optimal hyperplane maximizing the margin between classes. It achieves near-perfect separation, serving as an excellent baseline.

![Phase I Decision Boundary](plots/decision_boundary_phase_one.png)

### Phase II: Manual SVM Decision Boundaries

**Conclusion**: Our manual implementation successfully replicates the logic of a multi-class SVM.
1.  **First Boundary**: The model correctly identifies the "easy" split (Setosa vs. Rest).
2.  **Second Boundary**: It then effectively splits the remaining difficult pair (Versicolor vs. Virginica).
The combined result closely mimics the professional tool, proving the validity of our recursive "Divide and Conquer" approach and the correctness of our gradient descent solver.

![Phase II Decision Boundary](plots/decision_boundary_phase_two.png)

---

## ğŸš€ Installation

### Prerequisites

- Python 3.10 or higher
- pip package manager

### Setup

1.  **Clone or download the project**:
    ```bash
    cd C:\Ai_Expert\L23-svm
    ```

2.  **Create virtual environment** (recommended):
    ```bash
    python -m venv venv
    venv\Scripts\activate  # Windows
    # source venv/bin/activate  # Linux/Mac
    ```

3.  **Install dependencies**:
    ```bash
    pip install -r requirements.txt
    ```

4.  **Verify installation**:
    ```bash
    python -m src.run --help
    ```

---

## ğŸ’» Usage

### Run Full Pipeline

The main script executes both phases, generates plots, and logs results to `logs/execution.log`.

```bash
python -m src.run
```

### View Logs

Check the detailed execution logs:
```bash
type logs\execution.log  # Windows
# cat logs/execution.log # Linux/Mac
```

---

## ğŸ“ Project Structure

```
C:\Ai_Expert\L23-svm\
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ phase_one.py        # Scikit-learn implementation
â”‚   â”œâ”€â”€ phase_two.py        # Recursive multi-class logic
â”‚   â”œâ”€â”€ manual_svm.py       # Core Manual SVM solver (Gradient Descent)
â”‚   â”œâ”€â”€ plotting.py         # Visualization utilities
â”‚   â””â”€â”€ run.py              # Main entry point
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_phase_one.py   # Tests for Phase I
â”‚   â”œâ”€â”€ test_phase_two.py   # Tests for Phase II
â”‚   â””â”€â”€ test_manual_svm.py  # Tests for Manual SVM logic
â”œâ”€â”€ logs/                   # Execution logs
â”œâ”€â”€ plots/                  # Generated images
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ prd.md                  # Product Requirements Document
â””â”€â”€ README.md               # This file
```

---

## ğŸ“Š Results

### Performance Metrics

| Phase | Implementation | Accuracy | Notes |
| :--- | :--- | :--- | :--- |
| **I** | `sklearn.svm.SVC` | **~97.8%** | Linear Kernel, optimal baseline |
| **II** | `ManualSVM` | **~95.6%** | Gradient Descent, Recursive Strategy |

*Note: Results may vary slightly due to random initialization, but Phase II consistently matches Phase I performance.*

---

## ğŸ“ Educational Value

### Learning Objectives

This laboratory demonstrates:

1.  **Mathematical Optimization**: Implementing Gradient Descent manually to minimize Hinge Loss.
2.  **Algorithm Design**: Understanding how to break down a multi-class problem into smaller binary classification problems (Recursive Decomposition).
3.  **Benchmarking**: The importance of comparing custom implementations against established standards.
4.  **Data Scaling**: The critical role of `StandardScaler` in SVM convergence (addressed and fixed during development).

### Key Concepts

-   **Hinge Loss**: The loss function used for SVMs ($max(0, 1 - y_i \cdot (w \cdot x_i - b))$).
-   **Regularization**: The trade-off between margin size and classification error (controlled by $\lambda$).
-   **Decision Boundary**: The hyperplane that separates data classes.

---

## ğŸ§° Module Documentation

### Phase One (`src/phase_one.py`)
**Purpose**: Baseline implementation.
-   **Key Function**: `run_phase_one(...)`
-   **Lines**: ~32 (Concise usage of library)

### Manual SVM (`src/manual_svm.py`)
**Purpose**: Core mathematical solver.
-   **Key Class**: `ManualSVM`
-   **Logic**: Implements `fit` using a loop over iterations and samples to update weights `w` and bias `b`.
-   **Lines**: ~44 (Highly optimized)

### Phase Two (`src/phase_two.py`)
**Purpose**: Multi-class handling.
-   **Key Class**: `RecursiveBinaryClassifier`
-   **Logic**: Orchestrates two `ManualSVM` instances to solve the 3-class problem.
-   **Lines**: ~108 (Clean recursive structure)

### Plotting (`src/plotting.py`)
**Purpose**: Visualization.
-   **Functions**: `plot_iris_data`, `plot_decision_boundary`
-   **Lines**: ~44

---

## ğŸ”¬ Technical Implementation

### The Manual SVM Algorithm

Our `ManualSVM` implements the following update rule for Gradient Descent:

For each sample $x_i$ with label $y_i \in \{-1, 1\}$:

*   **Condition**: If $y_i \cdot (w \cdot x_i - b) \geq 1$ (Correctly classified with margin):
    *   $w = w - \alpha \cdot (2\lambda w)$
*   **Else** (Misclassified or within margin):
    *   $w = w - \alpha \cdot (2\lambda w - y_i \cdot x_i)$
    *   $b = b - \alpha \cdot y_i$

Where $\alpha$ is the learning rate and $\lambda$ is the regularization parameter.

---

## ğŸ§ª Testing

The project includes a comprehensive test suite using `unittest`.

**Run all tests:**
```bash
venv\Scripts\python -m unittest discover tests
```

**Test Coverage:**
*   `test_phase_one.py`: Verifies sklearn wrapper.
*   `test_manual_svm.py`: Verifies the core gradient descent logic on simple binary data.
*   `test_phase_two.py`: Verifies the recursive logic and scaling fix on 3-class data.

---

## ğŸ¤ Contributing

Contributions are welcome! Areas for enhancement:
1.  **Kernel Trick**: Implement non-linear kernels (RBF, Polynomial) manually.
2.  **Soft Margin**: implement soft margin with slack variables.

---

## ğŸ“ License

Â© Koby Lev - All Rights Reserved

This is an educational project for the AI Expert Program - Lesson 23.

---

## ğŸ™ Acknowledgments

-   **Course**: AI Expert Program
-   **Topic**: Support Vector Machines (SVM)
-   **Date**: December 2025